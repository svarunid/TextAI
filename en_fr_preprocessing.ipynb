{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c095813-31fc-47ed-9a75-62d74b8070e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500e0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parallel_corpus(src, tgt):\n",
    "    \"\"\"\n",
    "    Load data from the given path of a file in utf-8 format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src: str\n",
    "        Path of the source language file to load.\n",
    "    tgt: str\n",
    "        Path of the target language file to load.\n",
    "    \"\"\"\n",
    "    with open(src, encoding=\"utf-8\") as src_f, open(tgt, encoding=\"utf-8\") as tgt_f:\n",
    "        src = src_f.readlines()\n",
    "        tgt = tgt_f.readlines()\n",
    "        if len(src) != len(tgt):\n",
    "            raise ValueError(\"Number of records in source and target files are not equal\")\n",
    "        for i in range(len(src)):\n",
    "            if not src[i] and tgt[i]:\n",
    "                src.pop(i)\n",
    "                tgt.pop(i)\n",
    "        return src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b438d6-7940-46cf-9a2f-3a32cb38767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spm(iterable, prefix, vocab=2_000, sentence_size=50_000, model_type=\"bpe\"):\n",
    "    \"\"\"\n",
    "    Train a sentence-piece tokenizer.\n",
    "    The pad, unk, bos, eos tokens corresponds to 0, 1, 3 and 4 id respectively.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iterable: Sequence[str]\n",
    "        List of sequence to train the tokenizer.\n",
    "    prefix: str\n",
    "        A prefix for .model, .vocab files.\n",
    "    vocab: int, optional\n",
    "        Size of the vocabulary (default is 2_000)\n",
    "    sentence_size: int, optional\n",
    "        Size of sentences to sample for training. (default is 50_000)\n",
    "    model_type: str, optional\n",
    "        Type of model. Either \"bpe\" or \"unigram\". (default is \"bpe\")\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.train(sentence_iterator=iter(iterable), model_prefix=prefix, vocab_size=vocab,\n",
    "                                   model_type=\"bpe\", normalization_rule_name=\"identity\", \n",
    "                                   input_sentence_size=sentence_size, shuffle_input_sentence=True, \n",
    "                                   pad_id=0, unk_id=1, bos_id=2, eos_id=3, unk_surface='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efbc28c2-7dcf-4cde-9d8b-cdb8df99f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spm(path):\n",
    "    \"\"\"\n",
    "    Load a pretrained tokenizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to the model file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SentencePieceProcessor\n",
    "        A trained sentence-piece tokenizer.\n",
    "    \"\"\"\n",
    "    local_spm = spm.SentencePieceProcessor(model_file=f\"{path}.model\")\n",
    "    return local_spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd329483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(arr, train_size=0.8, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Splits a given list into train, validation and test sets. The sum of train &\n",
    "    validation sizes should not exceed 1. If the sum if < 1, test set has the remaining\n",
    "    elements of the array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: list\n",
    "        A list of sentences.\n",
    "    train_size: float, optional\n",
    "        Percentage of train set. (dafault is 0.8)\n",
    "    val_size: float, optional\n",
    "        Percentage of validation set (default is 0.1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (list, list, list)\n",
    "        A tuple of train, validation and test set.\n",
    "    \"\"\"\n",
    "    LEN = len(arr)\n",
    "    val_size = train_size + val_size\n",
    "    return arr[:int(LEN*train_size)], arr[int(LEN*train_size):int(LEN*val_size)], arr[int(LEN*val_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c90462-b2fb-4a43-81fd-c43e32f8b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr, form):\n",
    "    \"\"\"\n",
    "    Performs unicode noramlization on sentences.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    arr: str\n",
    "        A list of pairs of sentences.\n",
    "    form: str\n",
    "        Normalization form to use. NFD/NFC/NFKD/NFKC. (default is \"NFC\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of normalized sentences.\n",
    "    \"\"\"\n",
    "    return [unicodedata.normalize(form, seq) for seq in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e502f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "en, fr = load_parallel_corpus(\"./en_fr/en\", \"./en_fr/fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab8635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "en, fr = normalize(en, \"NFC\"), normalize(fr, \"NFC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431b7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xdev, Xte = data_split(en, train_size=0.7, val_size=0.15)\n",
    "ytr, ydev, yte = data_split(fr, train_size=0.7, val_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e3a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spm(Xtr, \"./tokenizer/en\", 5_000, sentence_size=1_000_000)\n",
    "train_spm(ytr, \"./tokenizer/fr\", 5_000, sentence_size=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14e15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_spm, fr_spm = load_spm(\"./tokenizer/en\"), load_spm(\"./tokenizer/fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f722f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xdev, Xte = en_spm.tokenize(Xtr, alpha=0.1), en_spm.tokenize(Xdev), en_spm.tokenize(Xte)\n",
    "ytr, ydev, yte = (\n",
    "    fr_spm.tokenize(ytr, add_bos=True, add_eos=True, alpha=0.1), \n",
    "    fr_spm.tokenize(ydev, add_bos=True, add_eos=True), \n",
    "    fr_spm.tokenize(yte, add_bos=True, add_eos=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4059f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data_splits/en_fr_train_tokenized.pickle\", 'wb') as f:\n",
    "    for data in zip(Xtr, ytr):\n",
    "        f.write(pickle.dumps(data))\n",
    "        f.write(b'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57432d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data_splits/en_fr_val_tokenized.pickle\", 'wb') as f:\n",
    "    for data in zip(Xdev, ydev):\n",
    "        f.write(pickle.dumps(data))\n",
    "        f.write(b'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77a41137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data_splits/en_fr_test_tokenized.pickle\", 'wb') as f:\n",
    "    for data in zip(Xte, yte):\n",
    "        f.write(pickle.dumps(data))\n",
    "        f.write(b'\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
