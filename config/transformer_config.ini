[model]
seed = 42
in_vocab = 5_000
out_vocab = 5_000
in_seq_len = 64
out_seq_len = 64
batch_size = 32
num_heads = 8
d_model = 256
d_ff = 1024
enc_layers = 3
dec_layers = 3

[training]
epochs = 10
lr = 0.001
data_path = ./data_splits/en_fr_train_tokenized.pickle
checkpoint_path = ./checkpoints/transfomer/
checkpoint_freq = 25
wandb = True
wandb_project = transformer
dataset = en_fr
wandb_notes = Initial run also will act as a baseline for future experiments
