[preprocessing]
train_size = 0.7
dev_size = 0.15
test_size = 0.15
shuffle = True
task = seq2seq

[tokenizer]
path = ./tokenizer/en_fr/
model = bpe
alpha = 0.1
sample_size = 1_000_000
save_path = ./splits/en_fr/

[seq2seq]
data_path = ./data/en_fr/
src = en
tgt = fr
vocab = 5_000
normalize = True