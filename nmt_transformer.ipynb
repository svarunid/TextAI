{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceaa3637",
   "metadata": {
    "id": "db414240",
    "raw_mimetype": "text/x-python"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import wandb\n",
    "import numpy as np\n",
    "from os import path\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "from etils import epath\n",
    "from pickle import loads\n",
    "from jax import lax, config\n",
    "import orbax.checkpoint as ocp\n",
    "from jax.tree_util import tree_map\n",
    "from itertools import islice, chain, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db58995",
   "metadata": {
    "id": "4db58995"
   },
   "outputs": [],
   "source": [
    "MODEL_DIM = EMB_SIZE = 256\n",
    "SEQ_LEN = 64\n",
    "VOCAB = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9940839e",
   "metadata": {
    "id": "9940839e"
   },
   "outputs": [],
   "source": [
    "N_HEADS = 8\n",
    "N_ENCODER_LAYERS = N_DECODER_LAYERS = 3\n",
    "INPUT_VOCAB = OUTPUT_VOCAB = VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eo1qjbWXROeI",
   "metadata": {
    "id": "eo1qjbWXROeI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 \n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aafb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update(\"jax_softmax_custom_jvp\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50505a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_iterator(path):\n",
    "    \"\"\"\n",
    "    Streaming itertor for pickled files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to the pickle file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Iterator\n",
    "        pickle object iterator\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        cur_lst = []\n",
    "        for pair in f:\n",
    "            if pair == b'\\n' and cur_lst[-1][-2:] == b'.\\n':\n",
    "                cur_lst.append(pair)\n",
    "                yield loads(b''.join(cur_lst))\n",
    "                cur_lst = []\n",
    "            else:\n",
    "                cur_lst.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "QhTLt3kLN1KH",
   "metadata": {
    "id": "QhTLt3kLN1KH"
   },
   "outputs": [],
   "source": [
    "def create_mask(arr):\n",
    "    \"\"\"\n",
    "    Creates a mask array in the same shape as the given a padded array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: jax.Array\n",
    "        Padded Array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    jax.Array\n",
    "        A mask array.\n",
    "    \"\"\"\n",
    "    return jnp.where(arr == 0, np.NINF, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qCUgSxL4NzZ1",
   "metadata": {
    "id": "qCUgSxL4NzZ1"
   },
   "outputs": [],
   "source": [
    "def pad_or_truncate(seq, size=32, pad=0):\n",
    "    \"\"\"\n",
    "    Pads and truncates a list tokens a specific length.\n",
    "\n",
    "    Parameters:\n",
    "    seq: list\n",
    "        A list of tokens.\n",
    "    size: int, optional\n",
    "        Max size of the sequence. (default is 32)\n",
    "    pad: int, optional\n",
    "        The ID to use for padding. (defualt is 0)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Truncated or padded list of tokens.\n",
    "    \"\"\"\n",
    "    return list(islice(chain(seq, repeat(pad)), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5292f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_batched_iterator(data, seq_len, batch_size=32):\n",
    "    \"\"\"\n",
    "    A dataloader for sequence to sequence tasks.\n",
    "    Loads data in batches.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Iterator\n",
    "        Pairs of Sequences.\n",
    "    seq_len: int\n",
    "        Max length of the sequence.\n",
    "    batch_size: int, optional\n",
    "        Size of a batch. (default is 32)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Iterator[tuple]\n",
    "        Batches of X, y and labels\n",
    "    \"\"\"\n",
    "    while batch := tuple(islice(data, batch_size)):\n",
    "        X = [x[0] for x in batch]\n",
    "        y = [x[1] for x in batch]\n",
    "        \n",
    "        Xbt = [pad_or_truncate(seq, seq_len) for seq in X]\n",
    "        y = [pad_or_truncate(seq, seq_len+1) for seq in y]\n",
    "        ybt = [x[:-1] for x in y]\n",
    "        labelbt = [x[1:] for x in y]\n",
    "        \n",
    "        yield Xbt, ybt, labelbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a860d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = seq2seq_batched_iterator(pickle_iterator(\"./data_splits/en_fr_train_tokenized.pickle\"), SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0c9bd6",
   "metadata": {
    "id": "8a0c9bd6"
   },
   "outputs": [],
   "source": [
    "class Linear(eqx.Module):\n",
    "    weights: jax.Array\n",
    "    bias: jax.Array\n",
    "    use_bias: bool = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, nin, nout, use_bias=False):\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        self.weights = init(key=key, shape=(nin, nout))\n",
    "        if use_bias:\n",
    "            self.bias = jnp.ones(nout)\n",
    "        else: \n",
    "            self.bias = None\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        x = x @ self.weights\n",
    "        if self.use_bias:\n",
    "            x = x + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe76caa0",
   "metadata": {
    "id": "fe76caa0"
   },
   "outputs": [],
   "source": [
    "class FFNN(eqx.Module):\n",
    "    layers: list\n",
    "    def __init__(self, key, nin, nout, nhidden, n_layers=2, use_bias=False):\n",
    "        keys = jax.random.split(key, num=n_layers)\n",
    "        layers = [\n",
    "            Linear(keys[0], nin, nhidden, use_bias)\n",
    "        ]\n",
    "        for i in range(1, n_layers-1):\n",
    "            layers.append(jax.nn.gelu)\n",
    "            layers.append(Linear(keys[i], nhidden, nhidden, use_bias))\n",
    "        if n_layers != 1:\n",
    "            layers.append(Linear(keys[-1], nhidden, nout, use_bias))\n",
    "        self.layers = layers\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6421e2be",
   "metadata": {
    "id": "6421e2be"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(eqx.Module):\n",
    "    wquery: jax.Array\n",
    "    wkey: jax.Array\n",
    "    wvalue: jax.Array\n",
    "    weights: jax.Array\n",
    "    n_heads: int = eqx.field(static=True)\n",
    "    dim_k: int = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, n_heads, dim):\n",
    "        if (dim % n_heads) != 0:\n",
    "            raise ValueError(\"Model dimensions must be a multiple of no. of heads\")\n",
    "        dim_k = dim // n_heads\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        wkey, qkey, kkey, vkey = jax.random.split(key, num=4)\n",
    "        self.weights = init(key=wkey, shape=(n_heads * dim_k, dim))\n",
    "        self.wquery = init(key=qkey, shape=(dim, dim))\n",
    "        self.wkey = init(key=kkey,shape=(dim, dim))\n",
    "        self.wvalue = init(key=vkey, shape=(dim, dim))\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_k = dim_k\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, query, key, value, mask):\n",
    "        query, key, value = query @ self.wquery, key @ self.wkey, value @ self.wvalue\n",
    "        query, key, value = [jnp.transpose(jnp.reshape(x, (-1, self.n_heads, self.dim_k)), (1, 0, 2)) \n",
    "                             for x in (query, key, value)]\n",
    "        if mask.ndim == 1:\n",
    "            mask = jnp.expand_dims(mask, axis=0)\n",
    "        # Attention Calculation\n",
    "        scaled_dot_prod = query @ jnp.transpose(key, (0, 2, 1)) / jnp.sqrt(query.shape[-1])\n",
    "        scaled_dot_prod = mask + scaled_dot_prod\n",
    "        attn = jax.nn.softmax(scaled_dot_prod) @ value\n",
    "        return jnp.reshape(jnp.transpose(attn, (1, 0, 2)), (-1, self.n_heads * self.dim_k)) @ self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d7ab91",
   "metadata": {
    "id": "85d7ab91"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(eqx.Module):\n",
    "    gamma: jax.Array\n",
    "    bias: jax.Array\n",
    "    eps: int = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, size, eps=1e-6):\n",
    "        self.gamma = jnp.ones(size)\n",
    "        self.bias = jnp.ones(size)\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        mean = jnp.mean(x, -1, keepdims=True)\n",
    "        std = jnp.std(x, -1, keepdims=True)\n",
    "        return (self.gamma * (x - mean) / (std + self.eps)) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8cf8498",
   "metadata": {
    "id": "b8cf8498"
   },
   "outputs": [],
   "source": [
    "class Encoder(eqx.Module):\n",
    "    emb: jax.Array\n",
    "    attn_layers: list\n",
    "    ff_layers:list\n",
    "    attn_norms: list\n",
    "    ff_norms: list\n",
    "    n_layers: int = eqx.field(static=True)\n",
    "    pe: jax.Array = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, n_layers, n_heads, dim, seq_len, vocab):\n",
    "        keys = jax.random.split(key, num=n_layers*2+1)\n",
    "        emb_key, attn_keys, ff_keys = keys[0], keys[1:n_layers+1], keys[n_layers+1:]\n",
    "        self.emb = jax.random.normal(emb_key, (vocab, dim))\n",
    "        # Self-Attention & Forward Layers\n",
    "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
    "        self.ff_layers = [FFNN(key, dim, dim, dim*2) for key in ff_keys]\n",
    "        # Layer Norms\n",
    "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        # Positional Encodings\n",
    "        pos = jnp.arange(seq_len)[:, jnp.newaxis]\n",
    "        div_term = 10_000 ** (2 * jnp.arange(0, dim, 2) / dim)\n",
    "        pe = jnp.empty((seq_len, dim))\n",
    "        pe = pe.at[:, 0::2].set(jnp.sin(pos / div_term))\n",
    "        pe = pe.at[:, 1::2].set(jnp.cos(pos / div_term))\n",
    "        self.pe = pe\n",
    "        # Static Arguments\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x, mask):\n",
    "        x = self.emb[x[...]]\n",
    "        x = x + self.pe\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.attn_norms[i](self.attn_layers[i](x, x, x, mask) + x)\n",
    "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3c48c3b",
   "metadata": {
    "id": "b3c48c3b"
   },
   "outputs": [],
   "source": [
    "class Decoder(eqx.Module):\n",
    "    emb: jax.Array\n",
    "    mask: jax.Array = eqx.field(static=True)\n",
    "    masked_attn_layers: list\n",
    "    attn_layers: list\n",
    "    ff_layers:list\n",
    "    masked_attn_norms: list\n",
    "    attn_norms: list\n",
    "    ff_norms: list\n",
    "    n_layers: int = eqx.field(static=True)\n",
    "    pe: jax.Array = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, n_layers, n_heads, dim, seq_len, vocab):\n",
    "        keys = jax.random.split(key, num=n_layers*3+1)\n",
    "        emb_key, attn_keys, ff_keys, masked_attn_keys = (\n",
    "            keys[0], keys[1:n_layers+1], keys[n_layers+1:n_layers*2+1], keys[n_layers*2+1:]\n",
    "        )\n",
    "        self.emb = jax.random.normal(emb_key, (vocab, dim))\n",
    "        self.mask = jnp.where(jnp.triu(jnp.ones((seq_len, seq_len)), 1) == 1, np.NINF, 0)\n",
    "        # Masked-Attention, Self-Attention & Forward Layers\n",
    "        self.masked_attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in masked_attn_keys]\n",
    "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
    "        self.ff_layers = [FFNN(key, dim, dim, dim*2) for key in ff_keys]\n",
    "        # Layer Norms\n",
    "        self.masked_attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        # Positional Encodings\n",
    "        pos = jnp.arange(seq_len)[:, jnp.newaxis]\n",
    "        div_term = 10_000 ** (2 * jnp.arange(0, dim, 2) / dim)\n",
    "        pe = jnp.empty((seq_len, dim))\n",
    "        pe = pe.at[:, 0::2].set(jnp.sin(pos / div_term))\n",
    "        pe = pe.at[:, 1::2].set(jnp.cos(pos / div_term))\n",
    "        self.pe = pe\n",
    "        # Static Arguments\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x, m, x_mask, m_mask):\n",
    "        x = self.emb[x[...]]\n",
    "        x = x + self.pe\n",
    "        x_mask = self.mask + x_mask\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.masked_attn_norms[i](self.masked_attn_layers[i](x, x, x, x_mask) + x)\n",
    "            x = self.attn_norms[i](self.attn_layers[i](x, m, m, m_mask) + x)\n",
    "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdf75eb5",
   "metadata": {
    "id": "fdf75eb5"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(eqx.Module):\n",
    "    encoder: eqx.Module\n",
    "    decoder: eqx.Module\n",
    "\n",
    "    def __init__(self, key, dim, enc_heads, enc_layers, dec_heads, dec_layers, in_vocab, out_vocab, seq_len):\n",
    "        enc_key, dec_key = jax.random.split(key, num=2)\n",
    "        self.encoder = Encoder(enc_key, enc_layers, enc_heads, dim, seq_len, in_vocab)\n",
    "        self.decoder = Decoder(dec_key, dec_layers, dec_heads, dim, seq_len, out_vocab)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, X, y, X_mask, y_mask):\n",
    "        m = self.encoder(X, X_mask)\n",
    "        h = self.decoder(y, m, y_mask, X_mask)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da4da317",
   "metadata": {
    "id": "da4da317"
   },
   "outputs": [],
   "source": [
    "class Transformer(eqx.Module):\n",
    "    enc_dec: eqx.Module\n",
    "    linear: eqx.Module\n",
    "\n",
    "    def __init__(self, key, dim, enc_heads, enc_layers, dec_heads, dec_layers, in_vocab, out_vocab, seq_len):\n",
    "        encdec_key, linear_key = jax.random.split(key)\n",
    "        self.enc_dec = EncoderDecoder(encdec_key, dim,\n",
    "                                      enc_heads, enc_layers, \n",
    "                                      dec_heads, dec_layers, \n",
    "                                      in_vocab, out_vocab, seq_len)\n",
    "        self.linear = Linear(linear_key, dim, out_vocab)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, X, y, X_mask, y_mask):\n",
    "        logits = self.enc_dec(X, y, X_mask, y_mask)\n",
    "        return jax.nn.softmax(self.linear(logits)) + 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2428b19d",
   "metadata": {
    "id": "2428b19d"
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "model = Transformer(key, MODEL_DIM, \n",
    "                    N_ENCODER_HEADS, N_ENCODER_LAYERS, \n",
    "                    N_DECODER_HEADS, N_DECODER_LAYERS, \n",
    "                    INPUT_VOCAB, OUTPUT_VOCAB, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "uvwdvhLUpiJK",
   "metadata": {
    "id": "uvwdvhLUpiJK"
   },
   "outputs": [],
   "source": [
    "scheduler = optax.warmup_cosine_decay_schedule(0.01, 1, 200, 2000, 0.5)\n",
    "optimizer = optax.adam(learning_rate=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fu2Yv1QEqEuk",
   "metadata": {
    "id": "fu2Yv1QEqEuk"
   },
   "outputs": [],
   "source": [
    "def predict(model, X, y, X_mask, y_mask):\n",
    "    return model(X, y, X_mask, y_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "oB9BgzuepqVy",
   "metadata": {
    "id": "oB9BgzuepqVy"
   },
   "outputs": [],
   "source": [
    "def loss(model, X, y, X_mask, y_mask, labels):\n",
    "    y_pred = jnp.log(predict(model, X, y, X_mask, y_mask))\n",
    "    y_pred = jnp.where(labels==0, 0, jnp.take(y_pred, labels, axis=-1))\n",
    "    count = jnp.count_nonzero(y_pred)\n",
    "    return -jnp.sum(y_pred)/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41dc5a88",
   "metadata": {
    "id": "41dc5a88"
   },
   "outputs": [],
   "source": [
    "def optim(model, optimizer, loss_fn, vectorize=True, in_axes=None, out_axes=None):\n",
    "    opt_state = optimizer.init(model)\n",
    "    grad = jax.value_and_grad(loss_fn)\n",
    "    if vectorize:\n",
    "        gradient = jax.vmap(grad, in_axes=in_axes, out_axes=out_axes)\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def step(model, opt_state, X, y, X_mask, y_mask, labels):\n",
    "        loss_value, grads = gradient(model, X, y, X_mask, y_mask, labels)\n",
    "        if vectorize:\n",
    "            loss_value = jnp.mean(loss_value)\n",
    "            grads = tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "        model = optax.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    return opt_state, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c355e415",
   "metadata": {
    "id": "c355e415"
   },
   "outputs": [],
   "source": [
    "opt_state, step = optim(model, optimizer, loss, in_axes=(None, 0, 0, 0, 0, 0), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a811ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./checkpoint/nmt-transformer/model.eqx\"\n",
    "opt_state_path = \"./checkpoint/nmt-transformer/opt_state.eqx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74e19f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.isfile(model_path) and path.isfile(opt_state_path):\n",
    "    model = eqx.tree_deserialise_leaves(model_path, model)\n",
    "    opt_state = eqx.tree_deserialise_leaves(opt_state_path, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce073eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"nmt-transformer\"\n",
    "\n",
    "run = wandb.init(\n",
    "    \"project\": PROJECT,\n",
    "    \"notes\": \"Initial run\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uqbtGzc-VaG9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqbtGzc-VaG9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "431559cf-3a92-4b12-95dc-d4a223c0c1cb"
   },
   "outputs": [],
   "source": [
    "for e in range(0, EPOCHS):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    for i, (Xbt, ybt, labelbt) in enumerate(data_loader):\n",
    "        if e*num_batches >= mngr.latest_step():\n",
    "            Xbt, ybt, labelbt = [jnp.array(x) for x in (Xbt, ybt, labelbt)]\n",
    "            Xmask, ymask = [create_mask(x) for x in (Xbt, ybt)]\n",
    "\n",
    "            model, opt_state, batch_loss = step(model, opt_state, Xbt, ybt, Xmask, ymask, labelbt)\n",
    "            total_loss += batch_loss\n",
    "            num_batches += 1\n",
    "\n",
    "            if num_batches % 25 == 0:\n",
    "                eqx.tree_serialise_leaves(model_path, model)\n",
    "                eqx.tree_deserialise_leaves(opt_state_path, opt_state)\n",
    "                print(f\"Batches trained: {num_batches} | Avg. Batch loss: {total_loss/num_batches}\")\n",
    "\n",
    "    epoch_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {e} | loss: {epoch_loss}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
