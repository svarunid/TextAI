{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db414240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db58995",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIM = EMB_SIZE = 128\n",
    "SEQ_LEN = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = jnp.arange(0, SEQ_LEN)\n",
    "positions = position / (1000 ** ((2*positions)/MODEL_DIM))\n",
    "positional_encodings = jnp.where(jnp.array([True if i%2 == 0 else False for i in range(16)]), \n",
    "                                 jnp.sin(positions),\n",
    "                                 jnp.cos(positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(eqx.Module):\n",
    "    def __init__(self, key, nin, nout):\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        self.weights = init(key, (nin, nout))\n",
    "        self.bias = jnp.ones(nout)\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(eqx.Module):\n",
    "    def __init__(self, key, nin, nout, nhidden, n_layers=2):\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        keys = jax.random.spilt(key, num=nlayers)\n",
    "        layers = [\n",
    "            Linear(keys[0], nin, nhidden)\n",
    "        ]\n",
    "        for i in range(1, nlayers-1):\n",
    "            layers.append(jax.nn.relu)\n",
    "            layers.append(Linear(keys[i], nhidden, nhidden))\n",
    "        if n_layers != 1:\n",
    "            layers.append(Linear(keys[-1], nhidden, nout))\n",
    "        self.layers = layers\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        for i in range(x):\n",
    "            x = self.layers(i)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(eqx.Module):\n",
    "    def __init__(self, key, dim, dim_k, dim_v, mask=None):\n",
    "        qkey, kkey, vkey = jax.random.split(key, num=3)\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        self.wquery = init(qkey, (dim, dim_k))\n",
    "        self.wkey = init(kkey, (dim, dim_k))\n",
    "        self.wvalue = init(vkey, (dim, dim_v))\n",
    "        self.mask = mask\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        query, key, value = x @ self.wquery, x @ self.wkey, x @ self.vkey\n",
    "        scaled_dot_prod = query @ key.T / jnp.sqrt(query.shape[1])\n",
    "        if self.mask is not None:\n",
    "            scaled_dot_prod = self.mask * scaled_dot_prod\n",
    "        return (jax.nn.softmax(scaled_dot_prod) @ value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutliHeadAttention(eqx.Module):\n",
    "    def __init__(self, key, n_heads, dim, mask=None):\n",
    "        if (dim % n_heads) != 0:\n",
    "            raise ValueError(\"Model dimensions must be a multiple of no. of heads\")\n",
    "        dim_k = dim_v = dim // n_heads\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        keys = jax.random.split(key, num=n_heads+1)\n",
    "        self.weights = init(keys[0], (h * dim_v, dim))\n",
    "        self.heads = [SelfAttention(k, dim, dim_k, dim_v, mask) for k in keys[1:]]\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        attn_vectors = []\n",
    "        for head in self.heads:\n",
    "            attn_vectors.append(head(x))\n",
    "        return jnp.hstack(attn_vectors) @ self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(eqx.Module):\n",
    "    def __init__(self, size, eps=1e-6):\n",
    "        self.gamma = jnp.ones(size)\n",
    "        self.bias = jnp.ones(bias)\n",
    "        self.eps = 1e-6\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        mean = jnp.mean(x, -1)\n",
    "        std = jnp.std(x, -1)\n",
    "        return (self.gamma * (x - mean) / (std + self.eps)) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(eqx.Module):\n",
    "    def __init__(self, key, n_layers, n_heads, dim):\n",
    "        keys = jax.random.split(key, num=n_layers*2)\n",
    "        attn_keys, ff_keys = keys[:n_layers], keys[n_layers:]\n",
    "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
    "        self.ff_layers = [FFNN(key, dim, dim, dim*4) for key in ff_keys]\n",
    "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        for i in range(n_layers):\n",
    "            x = self.attn_norms[i](self.attn_layers[i](x) + x)\n",
    "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c48c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(eqx.Module):\n",
    "    def __init__(self, key, n_layers, n_heads, dim):\n",
    "        keys = jax.random.split(key, num=n_layers*3)\n",
    "        mask = jnp.triu(jnp.ones((SEQ_LEN, dim)), -1) + 1e-9\n",
    "        attn_keys, ff_keys, masked_attn_keys = keys[:n_layers], keys[n_layers:n_layers*2], keys[n_layers*2]\n",
    "        self.masked_attn_layers = [MultiHeadAttention(key, n_heads, dim, mask=mask) for key in masked_attn_keys]\n",
    "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
    "        self.ff_layers = [FFNN(key, dim, dim, dim*4) for key in ff_keys]\n",
    "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.masked_attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        for i in range(n_layers):\n",
    "            x = self.masked_attn_norms[i](self.masked_attn_layers[i](x))\n",
    "            x = self.attn_norms[i](self.attn_layers[i](x) + x)\n",
    "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf75eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(eqx.Module):\n",
    "    def __init__(self, key, enc_heads, enc_layers, dec_layers, dec_heads, dim):\n",
    "        enc_key, dec_key = jax.random.split(key, num=2)\n",
    "        self.encoder = Encoder(enc_key, enc_layers, enc_heads)\n",
    "        self.decoder = Decoder(dec_key, dec_layers, dec_heads)\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
