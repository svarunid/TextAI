{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceaa3637",
   "metadata": {
    "id": "db414240",
    "raw_mimetype": "text/x-python"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import numpy as np\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "from pickle import loads\n",
    "from jax import lax, config\n",
    "from jax.tree_util import tree_map\n",
    "from itertools import islice, chain, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aafb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update(\"jax_softmax_custom_jvp\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50505a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_iterator(path):\n",
    "    \"\"\"\n",
    "    Streaming itertor for pickled files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to the pickle file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Iterator\n",
    "        pickle object iterator\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        cur_lst = []\n",
    "        for pair in f:\n",
    "            if pair == b'\\n' and cur_lst[-1][-2:] == b'.\\n':\n",
    "                cur_lst.append(pair)\n",
    "                yield loads(b''.join(cur_lst))\n",
    "                cur_lst = []\n",
    "            else:\n",
    "                cur_lst.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "QhTLt3kLN1KH",
   "metadata": {
    "id": "QhTLt3kLN1KH"
   },
   "outputs": [],
   "source": [
    "def create_mask(arr):\n",
    "    \"\"\"\n",
    "    Creates a mask array in the same shape as the given a padded array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: jax.Array\n",
    "        Padded Array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    jax.Array\n",
    "        A mask array.\n",
    "    \"\"\"\n",
    "    return jnp.where(arr == 0, np.NINF, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "qCUgSxL4NzZ1",
   "metadata": {
    "id": "qCUgSxL4NzZ1"
   },
   "outputs": [],
   "source": [
    "def pad_or_truncate(seq, size=32, pad=0):\n",
    "    \"\"\"\n",
    "    Pads and truncates a list tokens a specific length.\n",
    "\n",
    "    Parameters:\n",
    "    seq: list\n",
    "        A list of tokens.\n",
    "    size: int, optional\n",
    "        Max size of the sequence. (default is 32)\n",
    "    pad: int, optional\n",
    "        The ID to use for padding. (defualt is 0)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Truncated or padded list of tokens.\n",
    "    \"\"\"\n",
    "    return list(islice(chain(seq, repeat(pad)), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5292f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_batched_iterator(data, seq_len, batch_size=32):\n",
    "    \"\"\"\n",
    "    A dataloader for sequence to sequence tasks.\n",
    "    Loads data in batches.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Iterator\n",
    "        Pairs of Sequences.\n",
    "    seq_len: int\n",
    "        Max length of the sequence.\n",
    "    batch_size: int, optional\n",
    "        Size of a batch. (default is 32)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Iterator[tuple]\n",
    "        Batches of X, y and labels\n",
    "    \"\"\"\n",
    "    while batch := tuple(islice(data, batch_size)):\n",
    "        X = [x[0] for x in batch]\n",
    "        y = [x[1] for x in batch]\n",
    "        \n",
    "        Xbt = [pad_or_truncate(seq, seq_len) for seq in X]\n",
    "        y = [pad_or_truncate(seq, seq_len+1) for seq in y]\n",
    "        ybt = [x[:-1] for x in y]\n",
    "        labelbt = [x[1:] for x in y]\n",
    "        \n",
    "        yield Xbt, ybt, labelbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db58995",
   "metadata": {
    "id": "4db58995"
   },
   "outputs": [],
   "source": [
    "MODEL_DIM = EMB_SIZE = 256\n",
    "SEQ_LEN = 64\n",
    "VOCAB = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a860d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = seq2seq_batched_iterator(pickle_iterator(\"./data_splits/en_fr_train_tokenized.pickle\"), SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0c9bd6",
   "metadata": {
    "id": "8a0c9bd6"
   },
   "outputs": [],
   "source": [
    "class Linear(eqx.Module):\n",
    "    weights: jax.Array\n",
    "    bias: jax.Array\n",
    "    use_bias: bool = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, nin, nout, use_bias=False):\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        self.weights = init(key=key, shape=(nin, nout))\n",
    "        if use_bias:\n",
    "            self.bias = jnp.ones(nout)\n",
    "        else: \n",
    "            self.bias = None\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        x = x @ self.weights\n",
    "        if self.use_bias:\n",
    "            x = x + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe76caa0",
   "metadata": {
    "id": "fe76caa0"
   },
   "outputs": [],
   "source": [
    "class FFNN(eqx.Module):\n",
    "    layers: list\n",
    "    def __init__(self, key, nin, nout, nhidden, n_layers=2, use_bias=False):\n",
    "        keys = jax.random.split(key, num=n_layers)\n",
    "        layers = [\n",
    "            Linear(keys[0], nin, nhidden, use_bias)\n",
    "        ]\n",
    "        for i in range(1, n_layers-1):\n",
    "            layers.append(jax.nn.gelu)\n",
    "            layers.append(Linear(keys[i], nhidden, nhidden, use_bias))\n",
    "        if n_layers != 1:\n",
    "            layers.append(Linear(keys[-1], nhidden, nout, use_bias))\n",
    "        self.layers = layers\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6421e2be",
   "metadata": {
    "id": "6421e2be"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(eqx.Module):\n",
    "    wquery: jax.Array\n",
    "    wkey: jax.Array\n",
    "    wvalue: jax.Array\n",
    "    weights: jax.Array\n",
    "    n_heads: int = eqx.field(static=True)\n",
    "    dim_k: int = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, n_heads, dim):\n",
    "        if (dim % n_heads) != 0:\n",
    "            raise ValueError(\"Model dimensions must be a multiple of no. of heads\")\n",
    "        dim_k = dim // n_heads\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        wkey, qkey, kkey, vkey = jax.random.split(key, num=4)\n",
    "        self.weights = init(key=wkey, shape=(n_heads * dim_k, dim))\n",
    "        self.wquery = init(key=qkey, shape=(dim, dim))\n",
    "        self.wkey = init(key=kkey,shape=(dim, dim))\n",
    "        self.wvalue = init(key=vkey, shape=(dim, dim))\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_k = dim_k\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, query, key, value, mask):\n",
    "        query, key, value = query @ self.wquery, key @ self.wkey, value @ self.wvalue\n",
    "        query, key, value = [jnp.transpose(jnp.reshape(x, (-1, self.n_heads, self.dim_k)), (1, 0, 2)) for x in (query, key, value)]\n",
    "        if mask.ndim == 1:\n",
    "            mask = jnp.expand_dims(mask, axis=0)\n",
    "        # Attention Calculation\n",
    "        scaled_dot_prod = query @ jnp.transpose(key, (0, 2, 1)) / jnp.sqrt(query.shape[-1])\n",
    "        scaled_dot_prod = mask + scaled_dot_prod\n",
    "        attn = jax.nn.softmax(scaled_dot_prod) @ value\n",
    "        return jnp.reshape(jnp.transpose(attn, (1, 0, 2)), (-1, self.n_heads * self.dim_k)) @ self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d7ab91",
   "metadata": {
    "id": "85d7ab91"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(eqx.Module):\n",
    "    gamma: jax.Array\n",
    "    bias: jax.Array\n",
    "    eps: int = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, size, eps=1e-6):\n",
    "        self.gamma = jnp.ones(size)\n",
    "        self.bias = jnp.ones(size)\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        mean = jnp.mean(x, -1, keepdims=True)\n",
    "        std = jnp.std(x, -1, keepdims=True)\n",
    "        return (self.gamma * (x - mean) / (std + self.eps)) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8cf8498",
   "metadata": {
    "id": "b8cf8498"
   },
   "outputs": [],
   "source": [
    "class Encoder(eqx.Module):\n",
    "    emb: jax.Array\n",
    "    attn_layers: list\n",
    "    ff_layers:list\n",
    "    attn_norms: list\n",
    "    ff_norms: list\n",
    "    n_layers: int = eqx.field(static=True)\n",
    "    pe: jax.Array = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, n_layers, n_heads, dim, seq_len, vocab):\n",
    "        keys = jax.random.split(key, num=n_layers*2+1)\n",
    "        emb_key, attn_keys, ff_keys = keys[0], keys[1:n_layers+1], keys[n_layers+1:]\n",
    "        self.emb = jax.random.normal(emb_key, (vocab, dim))\n",
    "        # Self-Attention & Forward Layers\n",
    "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
    "        self.ff_layers = [FFNN(key, dim, dim, dim*2) for key in ff_keys]\n",
    "        # Layer Norms\n",
    "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        # Positional Encodings\n",
    "        pos = jnp.arange(seq_len)[:, jnp.newaxis]\n",
    "        div_term = 10_000 ** (2 * jnp.arange(0, dim, 2) / dim)\n",
    "        pe = jnp.empty((seq_len, dim))\n",
    "        pe = pe.at[:, 0::2].set(jnp.sin(pos / div_term))\n",
    "        pe = pe.at[:, 1::2].set(jnp.cos(pos / div_term))\n",
    "        self.pe = pe\n",
    "        # Static Arguments\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x, mask):\n",
    "        x = self.emb[x[...]]\n",
    "        x = x + self.pe\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.attn_norms[i](self.attn_layers[i](x, x, x, mask) + x)\n",
    "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c48c3b",
   "metadata": {
    "id": "b3c48c3b"
   },
   "outputs": [],
   "source": [
    "class Decoder(eqx.Module):\n",
    "    emb: jax.Array\n",
    "    mask: jax.Array = eqx.field(static=True)\n",
    "    masked_attn_layers: list\n",
    "    attn_layers: list\n",
    "    ff_layers:list\n",
    "    masked_attn_norms: list\n",
    "    attn_norms: list\n",
    "    ff_norms: list\n",
    "    n_layers: int = eqx.field(static=True)\n",
    "    pe: jax.Array = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, n_layers, n_heads, dim, seq_len, vocab):\n",
    "        keys = jax.random.split(key, num=n_layers*3+1)\n",
    "        emb_key, attn_keys, ff_keys, masked_attn_keys = keys[0], keys[1:n_layers+1], keys[n_layers+1:n_layers*2+1], keys[n_layers*2+1:]\n",
    "        self.emb = jax.random.normal(emb_key, (vocab, dim))\n",
    "        self.mask = jnp.where(jnp.triu(jnp.ones((seq_len, seq_len)), 1) == 1, np.NINF, 0)\n",
    "        # Masked-Attention, Self-Attention & Forward Layers\n",
    "        self.masked_attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in masked_attn_keys]\n",
    "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
    "        self.ff_layers = [FFNN(key, dim, dim, dim*2) for key in ff_keys]\n",
    "        # Layer Norms\n",
    "        self.masked_attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        # Positional Encodings\n",
    "        pos = jnp.arange(seq_len)[:, jnp.newaxis]\n",
    "        div_term = 10_000 ** (2 * jnp.arange(0, dim, 2) / dim)\n",
    "        pe = jnp.empty((seq_len, dim))\n",
    "        pe = pe.at[:, 0::2].set(jnp.sin(pos / div_term))\n",
    "        pe = pe.at[:, 1::2].set(jnp.cos(pos / div_term))\n",
    "        self.pe = pe\n",
    "        # Static Arguments\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x, m, x_mask, m_mask):\n",
    "        x = self.emb[x[...]]\n",
    "        x = x + self.pe\n",
    "        x_mask = self.mask + x_mask\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.masked_attn_norms[i](self.masked_attn_layers[i](x, x, x, x_mask) + x)\n",
    "            x = self.attn_norms[i](self.attn_layers[i](x, m, m, m_mask) + x)\n",
    "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdf75eb5",
   "metadata": {
    "id": "fdf75eb5"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(eqx.Module):\n",
    "    encoder: eqx.Module\n",
    "    decoder: eqx.Module\n",
    "\n",
    "    def __init__(self, key, dim, enc_heads, enc_layers, dec_heads, dec_layers, in_vocab, out_vocab, seq_len):\n",
    "        enc_key, dec_key = jax.random.split(key, num=2)\n",
    "        self.encoder = Encoder(enc_key, enc_layers, enc_heads, dim, seq_len, in_vocab)\n",
    "        self.decoder = Decoder(dec_key, dec_layers, dec_heads, dim, seq_len, out_vocab)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, X, y, X_mask, y_mask):\n",
    "        m = self.encoder(X, X_mask)\n",
    "        h = self.decoder(y, m, y_mask, X_mask)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da4da317",
   "metadata": {
    "id": "da4da317"
   },
   "outputs": [],
   "source": [
    "class Transformer(eqx.Module):\n",
    "    enc_dec: eqx.Module\n",
    "    linear: eqx.Module\n",
    "\n",
    "    def __init__(self, key, dim, enc_heads, enc_layers, dec_heads, dec_layers, in_vocab, out_vocab, seq_len):\n",
    "        encdec_key, linear_key = jax.random.split(key)\n",
    "        self.enc_dec = EncoderDecoder(encdec_key, dim,\n",
    "                                      enc_heads, enc_layers, \n",
    "                                      dec_heads, dec_layers, \n",
    "                                      in_vocab, out_vocab, seq_len)\n",
    "        self.linear = Linear(linear_key, dim, out_vocab)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, X, y, X_mask, y_mask):\n",
    "        logits = self.enc_dec(X, y, X_mask, y_mask)\n",
    "        return jax.nn.softmax(self.linear(logits)) + 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9940839e",
   "metadata": {
    "id": "9940839e"
   },
   "outputs": [],
   "source": [
    "N_ENCODER_HEADS = N_DECODER_HEADS = 8\n",
    "N_ENCODER_LAYERS = N_DECODER_LAYERS = 3\n",
    "INPUT_VOCAB = OUTPUT_VOCAB = VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2428b19d",
   "metadata": {
    "id": "2428b19d"
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "model = Transformer(key, MODEL_DIM, \n",
    "                    N_ENCODER_HEADS, N_ENCODER_LAYERS, \n",
    "                    N_DECODER_HEADS, N_DECODER_LAYERS, \n",
    "                    INPUT_VOCAB, OUTPUT_VOCAB, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eo1qjbWXROeI",
   "metadata": {
    "id": "eo1qjbWXROeI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 \n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "uvwdvhLUpiJK",
   "metadata": {
    "id": "uvwdvhLUpiJK"
   },
   "outputs": [],
   "source": [
    "scheduler = optax.warmup_cosine_decay_schedule(0.01, 1, 200, 2000, 0.5)\n",
    "optimizer = optax.adam(learning_rate=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fu2Yv1QEqEuk",
   "metadata": {
    "id": "fu2Yv1QEqEuk"
   },
   "outputs": [],
   "source": [
    "def predict(model, X, y, X_mask, y_mask):\n",
    "    return model(X, y, X_mask, y_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "oB9BgzuepqVy",
   "metadata": {
    "id": "oB9BgzuepqVy"
   },
   "outputs": [],
   "source": [
    "def loss(model, X, y, X_mask, y_mask, labels):\n",
    "    y_pred = jnp.log(predict(model, X, y, X_mask, y_mask))\n",
    "    y_pred = jnp.where(labels==0, 0, jnp.take(y_pred, labels, axis=-1))\n",
    "    count = jnp.count_nonzero(y_pred)\n",
    "    return -jnp.sum(y_pred)/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41dc5a88",
   "metadata": {
    "id": "41dc5a88"
   },
   "outputs": [],
   "source": [
    "def optim(model, optimizer, loss_fn, vectorize=True, in_axes=None, out_axes=None):\n",
    "    opt_state = optimizer.init(model)\n",
    "    grad = jax.value_and_grad(loss_fn)\n",
    "    if vectorize:\n",
    "        gradient = jax.vmap(grad, in_axes=in_axes, out_axes=out_axes)\n",
    "\n",
    "    def step(model, opt_state, X, y, X_mask, y_mask, labels):\n",
    "        loss_value, grads = gradient(model, X, y, X_mask, y_mask, labels)\n",
    "        if vectorize:\n",
    "            loss_value = jnp.mean(loss_value)\n",
    "            grads = tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "        model = optax.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    return opt_state, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c355e415",
   "metadata": {
    "id": "c355e415"
   },
   "outputs": [],
   "source": [
    "opt_state, step = optim(model, optimizer, loss, in_axes=(None, 0, 0, 0, 0, 0), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "uqbtGzc-VaG9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uqbtGzc-VaG9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "431559cf-3a92-4b12-95dc-d4a223c0c1cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches trained: 20 | Avg. Batch loss: 14.792669296264648\n",
      "Batches trained: 40 | Avg. Batch loss: 15.813653945922852\n",
      "Batches trained: 60 | Avg. Batch loss: 17.15995216369629\n",
      "Batches trained: 80 | Avg. Batch loss: 18.05078125\n",
      "Batches trained: 100 | Avg. Batch loss: 18.585277557373047\n",
      "Batches trained: 120 | Avg. Batch loss: 18.94161033630371\n",
      "Batches trained: 140 | Avg. Batch loss: 19.196125030517578\n",
      "Batches trained: 160 | Avg. Batch loss: 19.387012481689453\n",
      "Batches trained: 180 | Avg. Batch loss: 19.535478591918945\n",
      "200\n",
      "Batches trained: 200 | Avg. Batch loss: 19.654254913330078\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m Xbt, ybt, labelbt \u001b[38;5;241m=\u001b[39m [jnp\u001b[38;5;241m.\u001b[39marray(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (Xbt, ybt, labelbt)]\n\u001b[0;32m      6\u001b[0m Xmask, ymask \u001b[38;5;241m=\u001b[39m [create_mask(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (Xbt, ybt)]\n\u001b[1;32m----> 8\u001b[0m model, opt_state, batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXbt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mybt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelbt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[0;32m     10\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m, in \u001b[0;36moptim.<locals>.step\u001b[1;34m(model, opt_state, X, y, X_mask, y_mask, labels)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(model, opt_state, X, y, X_mask, y_mask, labels):\n\u001b[1;32m----> 8\u001b[0m     loss_value, grads \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vectorize:\n\u001b[0;32m     10\u001b[0m         loss_value \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(loss_value)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\api.py:1260\u001b[0m, in \u001b[0;36mvmap.<locals>.vmap_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m in_axes_flat \u001b[38;5;241m=\u001b[39m flatten_axes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap in_axes\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_tree, (in_axes, \u001b[38;5;241m0\u001b[39m), kws\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1258\u001b[0m axis_size_ \u001b[38;5;241m=\u001b[39m (axis_size \u001b[38;5;28;01mif\u001b[39;00m axis_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m               _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 1260\u001b[0m out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mbatching\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvmap out_axes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspmd_axis_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspmd_axis_name\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree(), out_flat)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\linear_util.py:190\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[0;32m    193\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[0;32m    194\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\api.py:749\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.value_and_grad_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m _check_scalar(ans)\n\u001b[0;32m    748\u001b[0m tree_map(partial(_check_output_dtype_grad, holomorphic), ans)\n\u001b[1;32m--> 749\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mvjp_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlax_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    750\u001b[0m g \u001b[38;5;241m=\u001b[39m g[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argnums, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m g\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\tree_util.py:353\u001b[0m, in \u001b[0;36m_HashableCallableShim.__call__\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m--> 353\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\api.py:2174\u001b[0m, in \u001b[0;36m_vjp_pullback_wrapper\u001b[1;34m(name, cotangent_dtypes, cotangent_shapes, io_tree, fun, *py_args_)\u001b[0m\n\u001b[0;32m   2169\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mshape(arg) \u001b[38;5;241m!=\u001b[39m ct_shape:\n\u001b[0;32m   2170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2171\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of cotangent input to vjp pullback function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be the same as the shape of corresponding primal input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2173\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2174\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree, ans)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\tree_util.py:353\u001b[0m, in \u001b[0;36m_HashableCallableShim.__call__\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m--> 353\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\interpreters\\ad.py:147\u001b[0m, in \u001b[0;36mvjp.<locals>.unbound_vjp\u001b[1;34m(pvals, jaxpr, consts, *cts)\u001b[0m\n\u001b[0;32m    145\u001b[0m cts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ct \u001b[38;5;28;01mfor\u001b[39;00m ct, pval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cts, pvals) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval\u001b[38;5;241m.\u001b[39mis_known())\n\u001b[0;32m    146\u001b[0m dummy_args \u001b[38;5;241m=\u001b[39m [UndefinedPrimal(v\u001b[38;5;241m.\u001b[39maval) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m jaxpr\u001b[38;5;241m.\u001b[39minvars]\n\u001b[1;32m--> 147\u001b[0m arg_cts \u001b[38;5;241m=\u001b[39m \u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(instantiate_zeros, arg_cts)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\interpreters\\ad.py:251\u001b[0m, in \u001b[0;36mbackward_pass\u001b[1;34m(jaxpr, reduce_axes, transform_stack, consts, primals_in, cotangents_in)\u001b[0m\n\u001b[0;32m    248\u001b[0m   cts_out \u001b[38;5;241m=\u001b[39m get_primitive_transpose(eqn\u001b[38;5;241m.\u001b[39mprimitive)(\n\u001b[0;32m    249\u001b[0m       params, call_jaxpr, invals, cts_in, cts_in_avals, reduce_axes)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m eqn\u001b[38;5;241m.\u001b[39mprimitive \u001b[38;5;129;01min\u001b[39;00m reducing_transposes:\n\u001b[1;32m--> 251\u001b[0m   cts_out \u001b[38;5;241m=\u001b[39m reducing_transposes[eqn\u001b[38;5;241m.\u001b[39mprimitive](\n\u001b[0;32m    252\u001b[0m       reduce_axes, cts_in, \u001b[38;5;241m*\u001b[39minvals, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meqn\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m   cts_out \u001b[38;5;241m=\u001b[39m get_primitive_transpose(eqn\u001b[38;5;241m.\u001b[39mprimitive)(\n\u001b[0;32m    255\u001b[0m       cts_in, \u001b[38;5;241m*\u001b[39minvals, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meqn\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\pjit.py:1699\u001b[0m, in \u001b[0;36m_pjit_transpose\u001b[1;34m(reduce_axes, cts_in, jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *primals_in)\u001b[0m\n\u001b[0;32m   1693\u001b[0m cts_out_treedef \u001b[38;5;241m=\u001b[39m cts_out_treedef_thunk()\n\u001b[0;32m   1694\u001b[0m transpose_out_shardings \u001b[38;5;241m=\u001b[39m prune_type(\n\u001b[0;32m   1695\u001b[0m     ad\u001b[38;5;241m.\u001b[39mZero,\n\u001b[0;32m   1696\u001b[0m     in_shardings,\n\u001b[0;32m   1697\u001b[0m     tree_unflatten(cts_out_treedef, [\u001b[38;5;28mobject\u001b[39m()] \u001b[38;5;241m*\u001b[39m cts_out_treedef\u001b[38;5;241m.\u001b[39mnum_leaves))\n\u001b[1;32m-> 1699\u001b[0m nz_cts_out \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals_and_nz_cts_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranspose_jaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranspose_in_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranspose_out_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprimals_and_nz_cts_in\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(cts_out_treedef, nz_cts_out)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\core.py:2657\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[1;34m(self, *args, **params)\u001b[0m\n\u001b[0;32m   2653\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[0;32m   2654\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   2655\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[0;32m   2656\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[1;32m-> 2657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\core.py:389\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[1;34m(self, trace, args, params)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m--> 389\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\interpreters\\batching.py:433\u001b[0m, in \u001b[0;36mBatchTrace.process_primitive\u001b[1;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[0;32m    431\u001b[0m   frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_frame(vals_in, dims_in)\n\u001b[0;32m    432\u001b[0m   batched_primitive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_primitive_batcher(primitive, frame)\n\u001b[1;32m--> 433\u001b[0m   val_out, dim_out \u001b[38;5;241m=\u001b[39m batched_primitive(vals_in, dims_in, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    434\u001b[0m src \u001b[38;5;241m=\u001b[39m source_info_util\u001b[38;5;241m.\u001b[39mcurrent()\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mmultiple_results:\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\pjit.py:1428\u001b[0m, in \u001b[0;36m_pjit_batcher\u001b[1;34m(insert_axis, spmd_axis_name, axis_size, axis_name, main_type, vals_in, dims_in, jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline)\u001b[0m\n\u001b[0;32m   1420\u001b[0m in_shardings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m   1421\u001b[0m     _pjit_batcher_for_sharding(i, axis_in, new_parts, mesh, aval\u001b[38;5;241m.\u001b[39mndim)\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m i\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis_in, i, aval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dims_in, in_shardings, new_jaxpr\u001b[38;5;241m.\u001b[39min_avals))\n\u001b[0;32m   1424\u001b[0m out_shardings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m   1425\u001b[0m     _pjit_batcher_for_sharding(o, axis_out, new_parts, mesh, aval\u001b[38;5;241m.\u001b[39mndim)\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m o\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis_out, o, aval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes_out, out_shardings, new_jaxpr\u001b[38;5;241m.\u001b[39mout_avals))\n\u001b[1;32m-> 1428\u001b[0m vals_out \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvals_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_jaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m  \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m  \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m  \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m  \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m  \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1438\u001b[0m resolved_axes_out \u001b[38;5;241m=\u001b[39m batching\u001b[38;5;241m.\u001b[39mresolve_ragged_axes_against_inputs_outputs(\n\u001b[0;32m   1439\u001b[0m     vals_in, vals_out, axes_out)\n\u001b[0;32m   1440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vals_out, resolved_axes_out\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\core.py:2657\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[1;34m(self, *args, **params)\u001b[0m\n\u001b[0;32m   2653\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[0;32m   2654\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   2655\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[0;32m   2656\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[1;32m-> 2657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\core.py:389\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[1;34m(self, trace, args, params)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m--> 389\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\core.py:869\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[1;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\pjit.py:1212\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[1;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[0;32m   1209\u001b[0m donated_argnums \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d]\n\u001b[0;32m   1210\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[0;32m   1211\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_argnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_registry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_explicit_sharding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Projects\\Data Science\\env\\lib\\site-packages\\jax\\_src\\api.py:113\u001b[0m, in \u001b[0;36m_nan_check_posthook\u001b[1;34m(fun, args, kwargs, output)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mmap\u001b[39m, unsafe_map \u001b[38;5;241m=\u001b[39m safe_map, \u001b[38;5;28mmap\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mzip\u001b[39m, unsafe_zip \u001b[38;5;241m=\u001b[39m safe_zip, \u001b[38;5;28mzip\u001b[39m\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_nan_check_posthook\u001b[39m(fun, args, kwargs, output):\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Hook function called by the C++ jit/pmap to perform NaN checking.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m   buffers \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    for i, (Xbt, ybt, labelbt) in enumerate(data_loader):\n",
    "        Xbt, ybt, labelbt = [jnp.array(x) for x in (Xbt, ybt, labelbt)]\n",
    "        Xmask, ymask = [create_mask(x) for x in (Xbt, ybt)]\n",
    "\n",
    "        model, opt_state, batch_loss = step(model, opt_state, Xbt, ybt, Xmask, ymask, labelbt)\n",
    "        total_loss += batch_loss\n",
    "        num_batches += 1\n",
    "\n",
    "        if num_batches % 20 == 0:\n",
    "            print(f\"Batches trained: {num_batches} | Avg. Batch loss: {total_loss/num_batches}\")\n",
    "\n",
    "    epoch_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {e} | loss: {epoch_loss}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
