{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db414240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import functools as ft\n",
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db58995",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIM = EMB_SIZE = 128\n",
    "SEQ_LEN = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = jnp.arange(0, SEQ_LEN)\n",
    "positions = position / (1000 ** ((2*positions)/MODEL_DIM))\n",
    "positional_encodings = jnp.where(jnp.array([True if i%2 == 0 else False for i in range(16)]), \n",
    "                                 jnp.sin(positions),\n",
    "                                 jnp.cos(positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(eqx.Module):\n",
    "    def __init__(self, key, nin, nout):\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        self.weights = init(key, (nin, nout))\n",
    "        self.bias = jnp.ones(nout)\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(eqx.Module):\n",
    "    def __init__(self, key, nin, nout, nhidden, n_layers=2):\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        keys = jax.random.spilt(key, num=nlayers)\n",
    "        layers = [\n",
    "            Linear(keys[0], nin, nhidden)\n",
    "        ]\n",
    "        for i in range(1, nlayers-1):\n",
    "            layers.append(jax.nn.gelu)\n",
    "            layers.append(Linear(keys[i], nhidden, nhidden))\n",
    "        if n_layers != 1:\n",
    "            layers.append(Linear(keys[-1], nhidden, nout))\n",
    "        self.layers = layers\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        for i in range(x):\n",
    "            x = self.layers(i)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(eqx.Module):\n",
    "    def __init__(self, key, dim, dim_k, dim_v):\n",
    "        qkey, kkey, vkey = jax.random.split(key, num=3)\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        self.wquery = init(qkey, (dim, dim_k))\n",
    "        self.wkey = init(kkey, (dim, dim_k))\n",
    "        self.wvalue = init(vkey, (dim, dim_v))\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, query, key, value, mask=None):\n",
    "        query, key, value = q @ self.wquery, k @ self.wkey, v @ self.vkey\n",
    "        scaled_dot_prod = query @ key.T / jnp.sqrt(query.shape[1])\n",
    "        lax.cond(mask is None, lambda x: x, lambda x: mask * x, scaled_dot_prod)\n",
    "        return (jax.nn.softmax(scaled_dot_prod) @ value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutliHeadAttention(eqx.Module):\n",
    "    def __init__(self, key, n_heads, dim):\n",
    "        if (dim % n_heads) != 0:\n",
    "            raise ValueError(\"Model dimensions must be a multiple of no. of heads\")\n",
    "        dim_k = dim_v = dim // n_heads\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        keys = jax.random.split(key, num=n_heads+1)\n",
    "        self.weights = init(keys[0], (h * dim_v, dim))\n",
    "        self.heads = [SelfAttention(k, dim, dim_k, dim_v) for k in keys[1:]]\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, query, key, value):\n",
    "        attn_vectors = []\n",
    "        for head in self.heads:\n",
    "            attn_vectors.append(head(query, key, value))\n",
    "        return jnp.hstack(attn_vectors) @ self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(eqx.Module):\n",
    "    def __init__(self, size, eps=1e-6):\n",
    "        self.gamma = jnp.ones(size)\n",
    "        self.bias = jnp.ones(bias)\n",
    "        self.eps = 1e-6\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        mean = jnp.mean(x, -1)\n",
    "        std = jnp.std(x, -1)\n",
    "        return (self.gamma * (x - mean) / (std + self.eps)) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(eqx.Module):\n",
    "    def __init__(self, key, n_layers, n_heads, dim):\n",
    "        keys = jax.random.split(key, num=n_layers*2)\n",
    "        attn_keys, ff_keys = keys[:n_layers], keys[n_layers:]\n",
    "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
    "        self.ff_layers = [FFNN(key, dim, dim, dim*4) for key in ff_keys]\n",
    "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        for i in range(n_layers):\n",
    "            x = self.attn_norms[i](self.attn_layers[i](x, x, x) + x)\n",
    "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c48c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(eqx.Module):\n",
    "    def __init__(self, key, n_layers, n_heads, dim):\n",
    "        keys = jax.random.split(key, num=n_layers*3)\n",
    "        mask = jnp.tril(jnp.ones((SEQ_LEN, dim)), 1) + 1e-9\n",
    "        attn_keys, ff_keys, masked_attn_keys = keys[:n_layers], keys[n_layers:n_layers*2], keys[n_layers*2]\n",
    "        self.masked_attn_layers = [MultiHeadAttention(key, n_heads, dim, mask=mask) for key in masked_attn_keys]\n",
    "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
    "        self.ff_layers = [FFNN(key, dim, dim, dim*4) for key in ff_keys]\n",
    "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.masked_attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x, m):\n",
    "        for i in range(n_layers):\n",
    "            x = self.masked_attn_norms[i](self.masked_attn_layers[i](x, x, x))\n",
    "            x = self.attn_norms[i](self.attn_layers[i](x, m, m) + x)\n",
    "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf75eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(eqx.Module):\n",
    "    def __init__(self, key, enc_heads, enc_layers, dec_layers, dec_heads, dim):\n",
    "        enc_key, dec_key = jax.random.split(key, num=2)\n",
    "        self.encoder = Encoder(enc_key, enc_layers, enc_heads, dim)\n",
    "        self.decoder = Decoder(dec_key, dec_layers, dec_heads, dim)\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, X, y):\n",
    "        m = self.encoder(X)\n",
    "        h = self.decoder(y, m)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4da317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(eqx.Module):\n",
    "    def __init__(self, key, dim, enc_heads, enc_layers, dec_layers, dec_heads, out_vocab):\n",
    "        encdec_key, linear_key = jax.random.split(key)\n",
    "        self.enc_dec = EncoderDecoder(encdec_key, enc_heads, enc_layers, dec_layers, dec_heads, dim)\n",
    "        self.linear = Linear(linear_key, dim, out_vocab)\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, X, y):\n",
    "        h = self.enc_dec(X, y)\n",
    "        return jax.nn.softmax(self.linear(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENCODER_HEADS = N_DECODER_HEADS = 8\n",
    "N_ENCODER_LAYERS = N_DECODER_LAYERS = 3\n",
    "INPUT_VOCAB = OUTPUT_VOCAB = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "model = Transformer(key, MODEL_DIM, N_ENCODER_HEADS, N_ENCODER_LAYERS, N_DECODER_HEADS, N_DECODER_LAYERS, OUTPUT_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def step(model, opt_state, batch, labels):\n",
    "    model = jax.vmap(model)\n",
    "    loss_value, grads = jax.value_and_grad(loss)(model, batch, labels)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = optax.apply_updates(model, updates)\n",
    "    return model, opt_state, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X_batch, y_batch) in enumerate(zip(X, y)):\n",
    "    model, opt_state, loss_value = step(model, opt_state, X_batch, y_batch)\n",
    "    if i % 100 == 0:\n",
    "        print(f'step {i}, loss: {loss_value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
