{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db414240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ae5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pickle\n",
    "\n",
    "with open(\"./en_fr/en_enc.pickle\", \"rb\") as en, open(\"./en_fr/fr_enc.pickle\", \"rb\") as fr:\n",
    "    en_enc = pickle.load(en)\n",
    "    fr_enc = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db58995",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIM = EMB_SIZE = 128\n",
    "SEQ_LEN = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = jnp.arange(0, 16, 1)\n",
    "positions = position / (1000 ** ((2*positions)/MODEL_DIM))\n",
    "positional_encodings = jnp.where(jnp.array([True if i%2 == 0 else False for i in range(16)]), \n",
    "                                 jnp.sin(positions),\n",
    "                                 jnp.cos(positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(eqx.Module):\n",
    "    def __init__(self, key, nin, nout):\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        self.weights = init(key, (nin, nout))\n",
    "        self.bias = jnp.ones(nout)\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + bias\n",
    "    \n",
    "class FFNN(eqx.Module):\n",
    "    def __init__(self, key, nin, nout, nhidden, nlayers):\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        keys = jax.random.spilt(key, num=nlayers)\n",
    "        layers = [\n",
    "            Linear(keys[0], nin, nhidden)\n",
    "        ]\n",
    "        for i in range(1, nlayers-1):\n",
    "            layers.append(jax.nn.relu)\n",
    "            layers.append(Linear(keys[i], nhidden, nhidden))\n",
    "        layers.append(Linear(keys[-1], nhidden, nout))\n",
    "        self.layers = layers\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        for i in range(x):\n",
    "            x = self.layers(i)\n",
    "        return x\n",
    "\n",
    "class SelfAttention(eqx.Module):\n",
    "    def __init__(self, key, dim, dim_k, dim_v, mask=None):\n",
    "        qkey, kkey, vkey = jax.random.split(key, num=3)\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        self.wquery = init(qkey, (dim, dim_k))\n",
    "        self.wkey = init(kkey, (dim, dim_k))\n",
    "        self.wvalue = init(vkey, (dim, dim_v))\n",
    "        self.mask = mask\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        query, key, value = x @ self.wquery, x @ self.wkey, x @ self.vkey\n",
    "        scaled_dot_prod = query @ key.T / jnp.sqrt(query.shape[1])\n",
    "        if self.mask is not None:\n",
    "            scaled_dot_prod = mask * scaled_dot_prod\n",
    "        return (jax.nn.softmax(scaled_dot_prod) @ value)\n",
    "    \n",
    "class MutliHeadAttention(eqx.Module):\n",
    "    def __init__(self, key, heads, dim):\n",
    "        if (dim % head) != 0:\n",
    "            raise ValueError(\"Model dimensions must be a multiple of no. of heads\")\n",
    "        dim_k = dim_v = dim // head\n",
    "        init = jax.nn.initializers.he_uniform()\n",
    "        keys = jax.random.split(key, num=heads+1)\n",
    "        self.weights = init(keys[0], (h * dim_v, dim))\n",
    "        self.heads = [SelfAttention(k, dim, dim_k, dim_v) for k in keys[1:]]\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
