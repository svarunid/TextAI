{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install equinox datasets sentencepiece"
      ],
      "metadata": {
        "id": "m3qQ6nuuzx6q"
      },
      "id": "m3qQ6nuuzx6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "db414240",
      "metadata": {
        "id": "db414240"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import equinox as eqx\n",
        "import optax\n",
        "import functools as ft\n",
        "from jax import lax\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "IiYQiVqGztdT"
      },
      "id": "IiYQiVqGztdT",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Trelis/tiny-shakespeare\", split=\"train\")"
      ],
      "metadata": {
        "id": "peExm4RE2_VS"
      },
      "id": "peExm4RE2_VS",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "SPLIT_PATTERN = re.compile(\"[\\\\n]{2,}\")\n",
        "def preprocess(seq):\n",
        "  seq[\"Text\"] = SPLIT_PATTERN.split(seq[\"Text\"])\n",
        "  return seq"
      ],
      "metadata": {
        "id": "RFpqHCuDMObo"
      },
      "id": "RFpqHCuDMObo",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess)"
      ],
      "metadata": {
        "id": "2aAfzV1DN7s5"
      },
      "id": "2aAfzV1DN7s5",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB=1000\n",
        "SEQ_LEN=56"
      ],
      "metadata": {
        "id": "TtTasvBqLwN0"
      },
      "id": "TtTasvBqLwN0",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train(sentence_iterator=(seq for row in dataset[\"Text\"] for seq in row),\n",
        "                               model_prefix=\"tiny-shakespeare\",\n",
        "                               model_type=\"bpe\",\n",
        "                               vocab_size=VOCAB,\n",
        "                               pad_id=0,\n",
        "                               unk_id=1,\n",
        "                               bos_id=2,\n",
        "                               eos_id=3\n",
        "                               )"
      ],
      "metadata": {
        "id": "wYHEuINw_IbR"
      },
      "id": "wYHEuINw_IbR",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = spm.SentencePieceProcessor(\"/content/tiny-shakespeare.model\")"
      ],
      "metadata": {
        "id": "9ch30sS2QstI"
      },
      "id": "9ch30sS2QstI",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4db58995",
      "metadata": {
        "id": "4db58995"
      },
      "outputs": [],
      "source": [
        "MODEL_DIM = EMB_SIZE = 96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cd5e5478",
      "metadata": {
        "id": "cd5e5478"
      },
      "outputs": [],
      "source": [
        "positions = jnp.arange(0, SEQ_LEN)\n",
        "positions = positions / (1000 ** ((2*positions)/MODEL_DIM))\n",
        "positional_encodings = jnp.where(jnp.array([True if i%2 == 0 else False for i in range(SEQ_LEN)]),\n",
        "                                 jnp.sin(positions),\n",
        "                                 jnp.cos(positions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8a0c9bd6",
      "metadata": {
        "id": "8a0c9bd6"
      },
      "outputs": [],
      "source": [
        "class Linear(eqx.Module):\n",
        "    weights: jax.Array\n",
        "    bias: jax.Array\n",
        "\n",
        "    def __init__(self, key, nin, nout):\n",
        "        init = jax.nn.initializers.he_uniform()\n",
        "        self.weights = init(key=key, shape=(nin, nout))\n",
        "        self.bias = jnp.ones(nout)\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, x):\n",
        "        return x @ self.weights + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fe76caa0",
      "metadata": {
        "id": "fe76caa0"
      },
      "outputs": [],
      "source": [
        "class FFNN(eqx.Module):\n",
        "    layers: list\n",
        "    def __init__(self, key, nin, nout, nhidden, n_layers=2):\n",
        "        keys = jax.random.split(key, num=n_layers)\n",
        "        layers = [\n",
        "            Linear(keys[0], nin, nhidden)\n",
        "        ]\n",
        "        for i in range(1, n_layers-1):\n",
        "            layers.append(jax.nn.gelu)\n",
        "            layers.append(Linear(keys[i], nhidden, nhidden))\n",
        "        if n_layers != 1:\n",
        "            layers.append(Linear(keys[-1], nhidden, nout))\n",
        "        self.layers = layers\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, x):\n",
        "        for i in range(len(self.layers)):\n",
        "            x = self.layers[i](x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7cc4a6b0",
      "metadata": {
        "id": "7cc4a6b0"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(eqx.Module):\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, query, key, value, mask):\n",
        "        scaled_dot_prod = query @ jnp.transpose(key, (0, 2, 1)) / jnp.sqrt(query.shape[-1])\n",
        "        scaled_dot_prod = mask + scaled_dot_prod\n",
        "        return (jax.nn.softmax(scaled_dot_prod) @ value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6421e2be",
      "metadata": {
        "id": "6421e2be"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(eqx.Module):\n",
        "    wquery: jax.Array\n",
        "    wkey: jax.Array\n",
        "    wvalue: jax.Array\n",
        "    weights: jax.Array\n",
        "    attn: eqx.Module\n",
        "    n_heads: int = eqx.field(static=True)\n",
        "    dim_k: int = eqx.field(static=True)\n",
        "\n",
        "    def __init__(self, key, n_heads, dim):\n",
        "        if (dim % n_heads) != 0:\n",
        "            raise ValueError(\"Model dimensions must be a multiple of no. of heads\")\n",
        "        dim_k = dim // n_heads\n",
        "        init = jax.nn.initializers.he_uniform()\n",
        "        wkey, qkey, kkey, vkey = jax.random.split(key, num=4)\n",
        "        self.weights = init(key=wkey, shape=(n_heads * dim_k, dim))\n",
        "        self.wquery = init(key=qkey, shape=(dim, dim))\n",
        "        self.wkey = init(key=kkey,shape=(dim, dim))\n",
        "        self.wvalue = init(key=vkey, shape=(dim, dim))\n",
        "        self.attn = SelfAttention()\n",
        "        self.n_heads = n_heads\n",
        "        self.dim_k = dim_k\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, query, key, value, mask):\n",
        "        query, key, value = query @ self.wquery, key @ self.wkey, value @ self.wvalue\n",
        "        query, key, value = [jnp.transpose(jnp.reshape(x, (-1, self.n_heads, self.dim_k)), (1, 0, 2)) for x in (query, key, value)]\n",
        "        mask = jnp.expand_dims(mask, axis=0)\n",
        "        return jnp.reshape(jnp.transpose(self.attn(query, key, value, mask), (1, 0, 2)), (-1, self.n_heads * self.dim_k)) @ self.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "85d7ab91",
      "metadata": {
        "id": "85d7ab91"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(eqx.Module):\n",
        "    gamma: jax.Array\n",
        "    bias: jax.Array\n",
        "    eps: int = eqx.field(static=True)\n",
        "\n",
        "    def __init__(self, size, eps=1e-6):\n",
        "        self.gamma = jnp.ones(size)\n",
        "        self.bias = jnp.ones(size)\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, x):\n",
        "        mean = jnp.mean(x, -1, keepdims=True)\n",
        "        std = jnp.std(x, -1, keepdims=True)\n",
        "        return (self.gamma * (x - mean) / (std + self.eps)) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b8cf8498",
      "metadata": {
        "id": "b8cf8498"
      },
      "outputs": [],
      "source": [
        "class Encoder(eqx.Module):\n",
        "    emb: jax.Array\n",
        "    attn_layers: list\n",
        "    ff_layers:list\n",
        "    attn_norms: list\n",
        "    ff_norms: list\n",
        "    n_layers: int = eqx.field(static=True)\n",
        "\n",
        "    def __init__(self, key, n_layers, n_heads, dim):\n",
        "        keys = jax.random.split(key, num=n_layers*2+1)\n",
        "        emb_key, attn_keys, ff_keys = keys[0], keys[1:n_layers+1], keys[n_layers+1:]\n",
        "        self.emb = jax.random.normal(emb_key, (VOCAB, EMB_SIZE))\n",
        "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
        "        self.ff_layers = [FFNN(key, dim, dim, dim*2) for key in ff_keys]\n",
        "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
        "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, x, mask):\n",
        "        x = self.emb[x]\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.attn_norms[i](self.attn_layers[i](x, x, x, mask) + x)\n",
        "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b3c48c3b",
      "metadata": {
        "id": "b3c48c3b"
      },
      "outputs": [],
      "source": [
        "class Decoder(eqx.Module):\n",
        "    emb: jax.Array\n",
        "    mask: jax.Array = eqx.field(static=True)\n",
        "    masked_attn_layers: list\n",
        "    attn_layers: list\n",
        "    ff_layers:list\n",
        "    masked_attn_norms: list\n",
        "    attn_norms: list\n",
        "    ff_norms: list\n",
        "    n_layers: int = eqx.field(static=True)\n",
        "\n",
        "    def __init__(self, key, n_layers, n_heads, dim):\n",
        "        keys = jax.random.split(key, num=n_layers*3+1)\n",
        "        emb_key, attn_keys, ff_keys, masked_attn_keys = keys[0], keys[1:n_layers+1], keys[n_layers+1:n_layers*2+1], keys[n_layers*2+1:]\n",
        "        self.emb = jax.random.normal(emb_key, (VOCAB, EMB_SIZE))\n",
        "        self.mask = jnp.where(jnp.triu(jnp.ones((SEQ_LEN, SEQ_LEN)), 1) == 1, np.NINF, 0)\n",
        "        self.masked_attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in masked_attn_keys]\n",
        "        self.attn_layers = [MultiHeadAttention(key, n_heads, dim) for key in attn_keys]\n",
        "        self.ff_layers = [FFNN(key, dim, dim, dim*2) for key in ff_keys]\n",
        "        self.attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
        "        self.ff_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
        "        self.masked_attn_norms = [LayerNorm(dim) for _ in range(n_layers)]\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, x, m, mask):\n",
        "        x = self.emb[x]\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.masked_attn_norms[i](self.masked_attn_layers[i](x, x, x, self.mask) + x)\n",
        "            x = self.attn_norms[i](self.attn_layers[i](x, m, m, mask) + x)\n",
        "            x = self.ff_norms[i](self.ff_layers[i](x) + x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fdf75eb5",
      "metadata": {
        "id": "fdf75eb5"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(eqx.Module):\n",
        "    encoder: eqx.Module\n",
        "    decoder: eqx.Module\n",
        "\n",
        "    def __init__(self, key, enc_heads, enc_layers, dec_heads, dec_layers, dim):\n",
        "        enc_key, dec_key = jax.random.split(key, num=2)\n",
        "        self.encoder = Encoder(enc_key, enc_layers, enc_heads, dim)\n",
        "        self.decoder = Decoder(dec_key, dec_layers, dec_heads, dim)\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, X, y, X_mask, y_mask):\n",
        "        m = self.encoder(X, X_mask)\n",
        "        h = self.decoder(y, m, y_mask)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "da4da317",
      "metadata": {
        "id": "da4da317"
      },
      "outputs": [],
      "source": [
        "class Transformer(eqx.Module):\n",
        "    enc_dec: eqx.Module\n",
        "    linear: eqx.Module\n",
        "\n",
        "    def __init__(self, key, dim, enc_heads, enc_layers, dec_heads, dec_layers, out_vocab):\n",
        "        encdec_key, linear_key = jax.random.split(key)\n",
        "        self.enc_dec = EncoderDecoder(encdec_key, enc_heads, enc_layers, dec_heads, dec_layers, dim)\n",
        "        self.linear = Linear(linear_key, dim, out_vocab)\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, X, y, X_mask, y_mask):\n",
        "        h = self.enc_dec(X, y, X_mask, y_mask)\n",
        "        return jax.nn.softmax(self.linear(h))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9940839e",
      "metadata": {
        "id": "9940839e"
      },
      "outputs": [],
      "source": [
        "N_ENCODER_HEADS = N_DECODER_HEADS = 8\n",
        "N_ENCODER_LAYERS = N_DECODER_LAYERS = 3\n",
        "INPUT_VOCAB = OUTPUT_VOCAB = VOCAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2428b19d",
      "metadata": {
        "id": "2428b19d"
      },
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "model = Transformer(key, MODEL_DIM, N_ENCODER_HEADS, N_ENCODER_LAYERS, N_DECODER_HEADS, N_DECODER_LAYERS, OUTPUT_VOCAB)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = optax.exponential_decay(0.05, 400, 0.5, transition_begin=50, end_value=0.0001)\n",
        "optimizer = optax.adam(learning_rate=scheduler)"
      ],
      "metadata": {
        "id": "uvwdvhLUpiJK"
      },
      "id": "uvwdvhLUpiJK",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, X, y, X_mask, y_mask):\n",
        "    return model(X, y, X_mask, y_mask)"
      ],
      "metadata": {
        "id": "fu2Yv1QEqEuk"
      },
      "id": "fu2Yv1QEqEuk",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(model, X, y, X_mask, y_mask):\n",
        "    y_pred = jnp.log(predict(model, X, y, X_mask, y_mask))\n",
        "    return -jnp.mean(y_pred[jnp.argmax(y, axis=-1)])"
      ],
      "metadata": {
        "id": "oB9BgzuepqVy"
      },
      "id": "oB9BgzuepqVy",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "41dc5a88",
      "metadata": {
        "id": "41dc5a88"
      },
      "outputs": [],
      "source": [
        "def optim(model, optimizer, loss_fn):\n",
        "    opt_state = optimizer.init(model)\n",
        "    vectorized_loss = jax.vmap(jax.value_and_grad(loss_fn), in_axes=(None, 0, 0, 0, 0), out_axes=(0))\n",
        "\n",
        "    def step(model, opt_state, X, y, X_mask, y_mask):\n",
        "        loss_value, grads = vectorized_loss(model, X, y, X_mask, y_mask)\n",
        "        loss_value = jnp.mean(loss_value)\n",
        "        grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
        "        updates, opt_state = optimizer.update(grads, opt_state, model)\n",
        "        model = optax.apply_updates(model, updates)\n",
        "        return model, opt_state, loss_value\n",
        "\n",
        "    return opt_state, step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c355e415",
      "metadata": {
        "id": "c355e415"
      },
      "outputs": [],
      "source": [
        "opt_state, step = optim(model, optimizer, loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice, chain, repeat\n",
        "\n",
        "def pad_or_truncate(seq, size=SEQ_LEN, pad=0):\n",
        "    return list(islice(chain(seq, repeat(pad)), size))"
      ],
      "metadata": {
        "id": "7xeLwgLa2zyO"
      },
      "id": "7xeLwgLa2zyO",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MINI_BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "qy9P7XI74-5E"
      },
      "id": "qy9P7XI74-5E",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(arr: jax.Array):\n",
        "    return jnp.where(arr == 0, np.NINF, 0)"
      ],
      "metadata": {
        "id": "fIwYFBlV-SDY"
      },
      "id": "fIwYFBlV-SDY",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch in dataset[\"Text\"]:\n",
        "        batch = tokenizer.encode(batch, add_bos=True, add_eos=True)\n",
        "        X = [seq[1:-1] for seq in batch]\n",
        "        y = batch\n",
        "        X, y = [jnp.array(list(map(pad_or_truncate, x))) for x in (X, y)]\n",
        "        X_mask, y_mask = [create_mask(x) for x in (X, y)]\n",
        "\n",
        "        model, opt_state, batch_loss = step(model, opt_state, X, y, X_mask, y_mask)\n",
        "        total_loss += batch_loss\n",
        "        num_batches += 1\n",
        "\n",
        "        if num_batches % 20 == 0:\n",
        "            print(f\"Batch: {num_batches} | Batch loss: {batch_loss}\")\n",
        "\n",
        "    epoch_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {e} | loss: {epoch_loss}\")"
      ],
      "metadata": {
        "id": "BnzmvXH-NA6K"
      },
      "id": "BnzmvXH-NA6K",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}